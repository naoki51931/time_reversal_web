# backend/models/pipeline_time_reversal.py
from __future__ import annotations
import os
from dataclasses import dataclass
from typing import List, Tuple

import torch
from PIL import Image
from diffusers.models import AutoencoderKL


# =====================================================================
# ğŸ”§ Utility
# =====================================================================

def _to_serializable(x):
    """JSONåŒ–ã§ããªã„torch.deviceãªã©ã‚’æ–‡å­—åˆ—åŒ–"""
    if isinstance(x, torch.device):
        return str(x)
    return x


def _make_multiple_of_8(w: int, h: int, max_side: int = 768) -> Tuple[int, int]:
    """
    ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ã¤ã¤ã€å„è¾ºã‚’8ã®å€æ•°ã«ä¸¸ã‚ã€æœ€å¤§è¾ºã‚’ max_side ã«æŠ‘ãˆã‚‹ã€‚
    """
    scale = min(1.0, max_side / max(w, h))
    w = int(round(w * scale))
    h = int(round(h * scale))
    w = max(8, (w // 8) * 8)
    h = max(8, (h // 8) * 8)
    return w, h


def _pil_to_tensor(pil: Image.Image, size: Tuple[int, int], device, dtype) -> torch.Tensor:
    """
    PIL â†’ æ¨™æº–åŒ–ãƒ†ãƒ³ã‚½ãƒ« [-1,1], shape: [1,3,H,W]
    """
    pil = pil.convert("RGB").resize(size, Image.LANCZOS)
    t = torch.tensor(list(pil.getdata()), dtype=torch.float32).view(pil.size[1], pil.size[0], 3)
    t = t.permute(2, 0, 1).unsqueeze(0) / 255.0  # [1,3,H,W]
    t = (t - 0.5) * 2.0  # [-1,1]
    return t.to(device=device, dtype=dtype)


def _tensor_to_pil(t: torch.Tensor) -> Image.Image:
    """æ¨™æº–åŒ–ãƒ†ãƒ³ã‚½ãƒ« [-1,1] â†’ PIL"""
    if t.ndim == 4:
        t = t[0]
    t = (t / 2 + 0.5).clamp(0, 1)
    t = (t * 255).round().to(torch.uint8)
    t = t.permute(1, 2, 0).contiguous().cpu().numpy()
    return Image.fromarray(t)


# =====================================================================
# ğŸ¬ Dataclass
# =====================================================================

@dataclass
class PipelineResult:
    status: str
    frames_generated: int
    frames: List[str]
    debug: dict


# =====================================================================
# ğŸ§© Pipeline core
# =====================================================================

class TimeReversalPipeline:
    """
    Diffusers å®Ÿåƒç‰ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå®‰å®šç‰ˆï¼‰:
      - Stable Diffusion 1.5 ã® VAE ã®ã¿ä½¿ç”¨ã€‚
      - 2æšã®ç”»åƒã®æ½œåœ¨è¡¨ç¾ã‚’ç·šå½¢/éç·šå½¢è£œé–“ã€‚
      - t0 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è£œé–“ã‚«ãƒ¼ãƒ–ã‚’åˆ¶å¾¡ï¼ˆåŠ é€Ÿãƒ»æ¸›é€Ÿï¼‰ã€‚
    """

    def __init__(
        self,
        device: str | torch.device = "cuda" if torch.cuda.is_available() else "cpu",
        model_id: str = "runwayml/stable-diffusion-v1-5",
        torch_dtype: torch.dtype | None = None,
        output_dir: str = "outputs",
        max_side: int = 768,
    ):
        self.device = torch.device(device)
        if torch_dtype is None:
            torch_dtype = torch.float16 if self.device.type == "cuda" else torch.float32
        self.dtype = torch_dtype
        self.output_dir = output_dir
        self.max_side = max_side
        os.makedirs(self.output_dir, exist_ok=True)

        # VAEå˜ä½“ãƒ­ãƒ¼ãƒ‰
        self.vae: AutoencoderKL = AutoencoderKL.from_pretrained(
            model_id, subfolder="vae", torch_dtype=self.dtype
        ).to(self.device)
        self.vae.eval()

        # SD VAE ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°
        self.scaling_factor: float = getattr(self.vae.config, "scaling_factor", 0.18215)
        print(f"[Init] TimeReversalPipeline ready on {self.device} (dtype={self.dtype})")

    # -----------------------------------------------------------------
    @torch.inference_mode()
    def __call__(
        self, image_1: Image.Image, image_2: Image.Image, M: int = 2, t0: float = 5.0
    ) -> PipelineResult:
        """
        ç”»åƒAã¨Bã®æ½œåœ¨ç©ºé–“è£œé–“ã§ M ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã€‚
        t0 ã§éç·šå½¢è£œé–“ã‚«ãƒ¼ãƒ–ã‚’èª¿æ•´ï¼ˆt0>0ã§å¾ŒåŠåŠ é€Ÿã€t0<0ã§å‰åŠåŠ é€Ÿï¼‰ã€‚
        """
        print("[Pipeline] Start processing (diffusers_full/vae_interpolate)")
        M = max(2, int(M))

        # --- ã‚µã‚¤ã‚ºèª¿æ•´ ---
        w0, h0 = image_1.size
        w1, h1 = image_2.size
        w, h = _make_multiple_of_8(min(w0, w1), min(h0, h1), self.max_side)

        # --- å‰å‡¦ç† ---
        img1 = _pil_to_tensor(image_1, (w, h), self.device, self.dtype)
        img2 = _pil_to_tensor(image_2, (w, h), self.device, self.dtype)
        print(f"[Pipeline] Preprocessed size: {(w, h)}")

        # --- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆæ½œåœ¨ã¸ï¼‰ ---
        posterior_1 = self.vae.encode(img1).latent_dist
        posterior_2 = self.vae.encode(img2).latent_dist
        z1 = posterior_1.sample() * self.scaling_factor
        z2 = posterior_2.sample() * self.scaling_factor
        print(f"[Pipeline] Latents shape: {tuple(z1.shape)}")

        # --- éç·šå½¢è£œé–“ã‚«ãƒ¼ãƒ–ï¼ˆt0åˆ©ç”¨ï¼‰ ---
        t = torch.linspace(0, 1, steps=M, device=self.device, dtype=self.dtype)
        if abs(t0) > 1e-6:
            # float â†’ tensor ã«å¤‰æ›ï¼ˆãƒ‡ãƒã‚¤ã‚¹ã¨dtypeã‚’åˆã‚ã›ã‚‹ï¼‰
            t0_tensor = torch.tensor(t0, device=self.device, dtype=self.dtype)
            alphas = (torch.exp(t * t0_tensor) - 1) / (torch.exp(t0_tensor) - 1)
        else:
            alphas = t

        frames: List[str] = []
        for i, a in enumerate(alphas):
            z = (1 - a) * z1 + a * z2
            x = self.vae.decode(z / self.scaling_factor).sample
            out_pil = _tensor_to_pil(x)
            out_path = os.path.join(self.output_dir, f"frame_{i:03d}.png")
            out_pil.save(out_path)
            frames.append(out_path)
            print(f"[Pipeline] Saved: {out_path}")

        debug = {
            "device": _to_serializable(self.device),
            "dtype": str(self.dtype).replace("torch.", ""),
            "size": [h, w],
            "latent_shape": [int(x) for x in z1.shape],
            "mode": "diffusers_full_vae_only",
            "t0": float(t0),
            "alphas": [float(a) for a in alphas.cpu()],
        }

        return PipelineResult(
            status="ok",
            frames_generated=len(frames),
            frames=frames,
            debug=debug,
        )

